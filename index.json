[{"authors":["admin"],"categories":null,"content":"Pietro Lesci is a recent graduate at Bocconi University and a former trainee at the European Central Bank in the Directorate of General Statistics. First of six, he is a passionate musician and a machine learning enthusiast.\n","date":1560902400,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1560902400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://lescipi.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Pietro Lesci is a recent graduate at Bocconi University and a former trainee at the European Central Bank in the Directorate of General Statistics. First of six, he is a passionate musician and a machine learning enthusiast.","tags":null,"title":"Pietro Lesci","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"https://lescipi.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"https://lescipi.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"https://lescipi.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"https://lescipi.github.io/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":null,"categories":null,"content":"","date":1561388951,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561388951,"objectID":"5ec7462a205ac475b3716bc3f67e42ef","permalink":"https://lescipi.github.io/project/thesis/","publishdate":"2019-06-24T17:09:11+02:00","relpermalink":"/project/thesis/","section":"project","summary":"Neural Process implementation in `Pytorch`","tags":null,"title":"Thesis","type":"project"},{"authors":["Pietro Lesci"],"categories":[],"content":" This blog post is an attempt at trying to explain the intuition behind MCMC sampling: specifically, a particular instance of the Metropolis-Hasting algorithm. Critically, we\u0026rsquo;ll be using TensorFlow-Probability code examples to explain the various concepts.\nThe Problem First, let\u0026rsquo;s import our modules. Note that we will use TensorFlow 2 Beta and we will use the TFP nightly distribution with works fine with TF2.\nimport numpy as np import tensorflow as tf import tensorflow_probability as tfp tfd = tfp.distributions import matplotlib.pyplot as plt import seaborn as sns tf.random.set_seed(1905) %matplotlib inline sns.set(rc={'figure.figsize':(9.3,6.1)}) sns.set_context('paper') sns.set_style('whitegrid') print(tf.__version__, tfp.__version__)  2.0.0 0.7.0  Let\u0026rsquo;s generate some data: 20 points from a Gaussian distribution centered around zero (the true Data-Generating Process that we want to discover from the 20 samples we can see). Note that in TFP the Gaussian distribution is parametrized by mean and standard deviation, not the variance.\ntrue_dgp = tfd.Normal(loc=0., scale=1.) observed = true_dgp.sample(20) sns.distplot(observed, kde=False) sns.despine();  We have some observations $x$.\nUsually (in parametric statistics) we assume a data-generating process, i.e. a model $P(x|\\theta)$, from which the data we see had been sampled \u0026ndash; note that $P$ is used to denote a probability density/mass function. Looking at the data, we come up \u0026ndash; somehow \u0026ndash; with the idea that a good model for our data is the Gaussian distribution. In other words, we assume that the data are normally distributed.\nThe model often depends on unknown parameters $\\theta$. They can be unknown because they are intrinsecally random or because simply we do not know them. A normal distribution has two parameters: the mean, $\\mu$, and the standard deviation, $\\sigma$. For simplicity, we assume we know $\\sigma=1$ and we want to make inference on $\\mu$ only, that is $\\theta \\equiv \\mu$.\nFrom a Bayesian viewpoint, we have to define a prior distribution for this parameter, i.e. $P(\\theta)$. Let\u0026rsquo;s also assume a normal distribution as a prior for $\\mu$. Our model can be written as follows (we assumed that the prior is a Gaussian distribution with mean 4 and stardard deviation 2)\n$$x_i|\\mu \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\mu, \\sigma=1)$$\n$$\\mu \\sim \\mathcal{N}(\\mu_0 = 4, \\sigma_0 = 2)$$\nIn the Bayesian Stat lingo, this way of writing the model derives from the fact that knowing nothing about the joint distribution of the $x$\u0026rsquo;s\u0026rsquo; we can assume exchangeability. By the De Finetti\u0026rsquo;s Theorem we arrive to the above formulation. Anyway, this goes beyond the scope of this blog post. For more information on Bayesian Analysis look at (the bible) Gelman et al. book.\n# prior mu_0, sigma_0 = 4., 2. prior = tfd.Normal(mu_0, sigma_0) # likelihood mu, sigma = prior.sample(1), 1. # use a sample from the prior as guess for mu likelihood = tfd.Normal(mu, sigma)   Digression: note that actually what I called likelihood, is the likelihood for one specific datapoint \u0026ndash; call it $\\mathrm{likelihood}_i, i=1,\\dots,N$. The \u0026ldquo;proper\u0026rdquo; likelihood function is (given that we have an $i.i.d.$ sample) equal to the product of the \u0026ldquo;per-datapoint\u0026rdquo; likelihoods\n$$\\mathcal{L}(\\mu; x) = \\prod_{i=1}^n \\underbrace{\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\left\\{-\\frac{(x_i-\\mu)^2}{2\\sigma^2}\\right\\}}_{\\mathcal{L}(\\mu; x_i)}$$\nWhen we consider the loglikelihood, obviously, for an $i.i.d.$ sample, the loglikelihood is the sum of the individual \u0026ldquo;per-datapoint\u0026rdquo; likelihoods\n$$\\mathcal{l}(\\mu; x) = \\sum_{i=1}^n \\mathcal{l}(\\mu; x_i)$$\nUsually, the likelihood is denoted by $\\mathcal{L}(\\mu; x)$ or $p(x|\\mu)$.\n Since we do not know the mean of the Gaussian distribution which generated the data, we use a sample from the prior distribution as a guess for $\\mu$ in order to be able to draw it (we need a value), the likelihood has a mean similar to that of the prior distribution.\nIn the graph below, I plot both the prior and the likelihood, as well as the true data-generating process with the data plotted as a rug\nsns.rugplot(observed, linewidth=2, height=0.1) sns.distplot(prior.sample(10**5)) sns.distplot(likelihood.sample(10**5)) sns.distplot(true_dgp.sample(10**5)) sns.despine() plt.legend(labels=['Data', 'Prior','Likelihood', 'True DGP']) plt.xlim(-5, 7);  In the Bayesian framework, inference, i.e. knowing something more about the unknown parameters, is solved by the Bayes formula\n$$P(\\theta|x)=\\frac{P(x|\\theta)P(\\theta)}{P(x)}$$\nThe posterior distribution $P(\\theta|x)$ \u0026ndash; that is, what we know about our model parameters $\\theta$ after having seen thet data $x$ \u0026ndash; is our quantity of interest.\nTo compute it, we multiply the prior $P(\\theta)$ (what we think about $\\theta$ before we have seen any data) and the likelihood $P(x|\\theta)$, dividing by the evidence $P(x)$ (a.k.a. marginal likelihood).\nHowever, let\u0026rsquo;s take a closer look at this last term: the denominator, $P(x)$. We do not observe it, but we can compute this quantity by integrating over all possible parameter values:\n$$P(x)=\\int_\\Theta P(x,\\theta) \\ d\\theta$$\nThis is the key difficulty with the Bayes formula \u0026ndash; while the formula looks pretty enough, for even slightly non-trivial models we cannot compute the posterior in a closed-form way.\nNOTE: $P(x)$ is a normalizing constant. Up to this normalizing constant, we know exactly how the unnormalized posterior distribution looks like, i.e.\n$$P(\\theta|x) \\propto P(x|\\theta) P(\\theta)$$\n(where $\\propto$ mean \u0026ldquo;proportional to\u0026rdquo;). Since we defined both terms on the rhs, we DO know how to sample from the unnormalized posterior distribution\nFurthermore, by the product rule \u0026ndash; $P(A, B) = P(A|B) P(B)$ \u0026ndash; we can write\n$$P(\\theta|x) \\propto P(x, \\theta)$$\nmeaning that the unnormalized posterior is proportional to the joint distribution of $x$ and $\\theta$.\nBack to the example. The prior distribution we defined is convenient because we can actually compute the posterior distribution analytically. That\u0026rsquo;s because for a normal likelihood with known standard deviation, the normal prior distribution for $\\mu$ is conjugate, i.e. our posterior distribution will belong to the same family of distributions of the prior. Therefore, we know that our posterior distribution for $\\mu$ is also normal. For a mathematical derivation see here.\nLet\u0026rsquo;s define a function which computes the updates for the parameters of the posterior distribution analytically\ndef get_param_updates(data, sigma, prior_mu, prior_sigma): #sigma is known n = len(data) sigma2 = sigma**2 prior_sigma2 = prior_sigma**2 x_bar = tf.reduce_mean(data) post_mu = ((sigma2 * prior_mu) + (n * prior_sigma2 * x_bar)) / ((n * prior_sigma2) + (sigma2)) post_sigma2 = (sigma2 * prior_sigma2) / ((n * prior_sigma2) + sigma2) post_sigma = tf.math.sqrt(post_sigma2) return post_mu, post_sigma  # posterior mu_n, sigma_n = get_param_updates(observed, sigma=1, prior_mu=mu_0, prior_sigma=sigma_0) posterior = tfd.Normal(mu_n, sigma_n, name='posterior')  In the graph below, I plot both the prior and the posterior distributions. Furthermore, I plot the likelihood both with mean set to a sample from the prior and a sample from the posterior\nsns.distplot(prior.sample(10**5)) sns.distplot(posterior.sample(10**5)) sns.distplot(likelihood.sample(10**5)) sns.distplot(tfd.Normal(posterior.sample(1), 1.).sample(10**5)) sns.despine() plt.legend(labels=['Prior','Posterior', 'Likelihood (prior)', 'Likelihood (Posterior)']) plt.xlim(-5, 7);  This shows our quantity of interest (orange): the probability of $\\mu$\u0026rsquo;s values after having seen the data, taking our prior information into account.\nThe important thing to acknowledge is that, without conjugacy, we would not even be capable of sketching the posterior distribution: we would not know its shape at all. Let\u0026rsquo;s assume, however, that our prior was not conjugate and we could not solve this by hand \u0026ndash; which is often the case.\nApproximation methods When we do not have access to the analytic form of the posterior distribution we can resort to MCMC methods. The basic idea is that we can find strategies to sample from the posterior distribution, even if we cannot \u0026ldquo;write it down\u0026rdquo;. These samples are then used to approximate the posterior distribution. One simple strategy to get samples from the posterior distribution is the Rejection Sampling algorithm.\nRejection Sampling The basic idea of rejection sampling is to sample from an instrumental distribution and reject samples that are \u0026ldquo;unlikely\u0026rdquo; under the target distribution. Here we consider a very specific instance of rejection sampling: the Naive Rejection Sampling.\nSuppose that you can sample from a joint distribution $P(X, \\theta)$ (where $X$ is random as well) \u0026ndash; we have seen that we can sample from it since using the product rule we get $P(X, \\theta) = P(X|\\theta) P(\\theta)$, which are both defined by us, so we know how to sample from them!\nWe are interested in sampling $\\theta$ from the conditional distribution $P(\\theta|X = x)$, for some fixed values of $x$ \u0026ndash; i.e. the observed data.\nThe Naive Rejection Sampling algorithm works as follows:\n Sample $\\theta$ from the prior $P(\\theta)$ and $X$ from the likelihood $P(X|\\theta)$\n If $X = x$ (the observed data) , accept $\\theta$ as a sample from the posterior $P(\\theta|X = x)$ , otherwise return to (1) and repeat\n  Each time you return to step 1, the samples of $\\theta$ are independent from the previous ones.\nPros: step 1 is often practical because both the prior and the likelihood are often easy-to-sample distributions. Cons: the clear shortcoming is that step 2 can be very unlikely and thus we will very rarely (if ever) accept the candidate sample $\\theta$.\nThis simple implementation of rejection sampling is enough to provide some intuition and motivates the use of more sophisticated and robust sampling algorithms based on Markov chains.\nMCMC: The Random-Walk Metropolis-Hasting algorithm There is a large family of algorithms that perform MCMC. Most of these algorithms can be expressed at a high level as follows:\n Start at current position (i.e. a value for $\\theta$, say $\\theta^{(1)}$)\n Propose moving to a new position (say, $\\theta^\\star$)\n Accept/Reject the new position based on the position\u0026rsquo;s adherence to the data and prior distributions\n  If you accept: Move to the new position (i.e. $\\theta^{(2)}=\\theta^\\star$) and return to Step 1 Else: Do not move to new position. Return to Step 1.  After a large number of iterations, return all accepted positions.\n  Based on how you implement the above steps you get the various MCMC algorithm. Here we will review the Random-Walk Metropolis-Hasting algorithm.\nAs we have seen, the main drawback of the rejection sampling is that it is not efficient \u0026ndash; it is unlikely to get exactly $X = x$, especially when it is high-dimensional.\nOne way around this problem is to allow for \u0026ldquo;local updates\u0026rdquo;, i.e. let the proposed value depend on the last accepted value (here is the part where Markov Chains enter the scene).\nThis makes it easier to come up with a suitable (conditional) proposal, however at the price of yielding a Markov chain, instead of a sequence of independent realizations \u0026ndash; putting it simply, a sequence of random variables is a Markov Chain if the future state only depends on the present state.\nAt first, you find a starting position (can be randomly chosen), lets fix it arbitrarily to\nmu_current = 2.  The critical point is how you propose the new position (that\u0026rsquo;s the Markov part). You can be very naive or very sophisticated about how you come up with that proposal. The RW-MH algorithm is very naive and just takes a sample from a Gaussian distribution (or whatever simmetric distribution you like) centered on the current value with a certain standard deviation, usually called proposal width that will determine how far you propose jumps. In other words, the RW-MH proposes a new $\\theta^\\star$ according to\n$$\\theta^\\star = \\theta_{s} + \\varepsilon, \\quad \\varepsilon \\sim g$$\nwhere $g$ may be any simmetric distribution. Usually, $g = \\mathcal{N}(0, \\tau)$, so that the proposed new value, $\\theta^\\star$, is simply a draw from $\\mathcal{N}(\\theta_{s}, \\tau)$.\nproposal_width = 1. mu_proposal = tfd.Normal(mu_current, proposal_width).sample()  Next, you evaluate whether that\u0026rsquo;s a good place to jump to or not. To evaluate if it is good, you compute the ratio\n$$\\rho = \\frac{P(\\theta^\\star|x)}{P(\\theta_s|x)} = \\frac{P(x|\\theta^\\star) P(\\theta^\\star)/P(x)}{P(x|\\theta_s) P(\\theta_s)/P(x)} = \\frac{P(x, \\theta^\\star)}{P(x, \\theta_s)}$$\nHere is the trick: the normalizing constants cancel out. We only have to compute the numerator of the Bayes\u0026rsquo; formula, that is the product of likelihood and prior. We have seen that it is the same as computing the joint probability distribution \u0026ndash; usually, we compute the log joint probability in practise \u0026ndash; of the data and the parameter values. TFP performs probabilistic inference by evaluating the model parameters using a joint_log_prob function that the user as to provide (which we define below).\nThen,\n If $\\rho\\geq1$, set $\\theta^{s+1}=\\theta^\\star$\n If $\\rhou$; if it is you accept the proposal)\n  To sum up, we accept a proposed move to $\\theta^\\star$ whenever the density of the (unnormalzied) joint distribution evaluated at $\\theta^\\star$ is larger than the value of the unnormalized joint distribution evaluated at $\\theta_s$ \u0026ndash; so $\\theta$ will more often be found in places where the unnormalized joint distribution is denser.\nIf this was all we accepted, $\\theta$ would get stuck at a local mode of the target distribution, so we also accept occasional moves to lower density regions.\nNOTE: The model we define enters the inference scheme only when we evaluate the proposal. In other words, the model we define is important made explicit in the definition of the joint_log_prob function, that is\njoint_log_prob = model definition  Let\u0026rsquo;s now define the joint log probability of the normal model above.\n# definition of the joint_log_prob to evaluate samples def joint_log_prob(data, proposal): prior = tfd.Normal(mu_0, sigma_0, name='prior') likelihood = tfd.Normal(proposal, sigma, name='likelihood') return prior.log_prob(proposal) + tf.reduce_mean(likelihood.log_prob(data))  Let\u0026rsquo;s evaluate the proposal above, i.e. mu_proposal\n# compute acceptance ratio p_accept = joint_log_prob(observed, mu_proposal) / joint_log_prob(observed, mu_current) print('Acceptance probability:', p_accept.numpy())  Acceptance probability: 1.0373609  It is more than 1, therefore we accept directly. Imagine that p_accept was $0.8$, then we would have drawn a sample from the uniform distribution and check the following\nif p_accept \u0026gt; tfd.Uniform().sample(): mu_current = mu_proposal print('Proposal accepted') else: print('Proposal not accepted')  Proposal accepted  At this point we would restart the process again.\nTFP implementation In TFP the algorithm is implemented as follows.\nFirst we define how the step should be taken, i.e. how the proposal should be made. Since we are implementing the RW-MH algorithm we use the function tfp.mcmc.RandomWalkMetropolis. It takes as argument the unnormalized join distribution that it will use to compute the acceptance ratio. The only thing we have to remenber is that we have to \u0026ldquo;lock the data\u0026rdquo; or \u0026ldquo;define a closure\u0026rdquo; over our joint_log_prob function. In other words, fix the data input of the function joint_log_prob\n# define a closure on joint_log_prob def unnormalized_log_posterior(proposal): return joint_log_prob(data=observed, proposal=proposal)  Now we can pass the unnormalized_log_posterior as the argument of the function which implements the step\nrwm = tfp.mcmc.RandomWalkMetropolis( target_log_prob_fn=unnormalized_log_posterior )  Secondly, we have to define the initial state of the chain, say $\\theta_0$. We choose this arbitrarily.\ninitial_state = tf.constant(0., name='initial_state')  Finally, we can sample the chain with the function tf.mcmc.sample_chain, which returns the samples (named trace usign the usual stat lingo) and some additional information regarding the procedure implemented (kernel_results)\ntrace, kernel_results = tfp.mcmc.sample_chain( num_results=10**5, num_burnin_steps=5000, current_state=initial_state, num_steps_between_results=1, kernel=rwm, parallel_iterations=1 )  However, to take full advantage of TF, we will enclose this sampling process into a function and we will decorate it with tf.function\n@tf.function def run_chain(): samples, kernel_results = tfp.mcmc.sample_chain( num_results=10**5, num_burnin_steps=5000, current_state=initial_state, kernel=rwm, parallel_iterations=1, trace_fn=lambda _, pkr: pkr) return samples, kernel_results   Note: To print the code generated by tf.function on fn, use\ntf.autograph.to_code(fn.python_function)   trace, kernel_results = run_chain() plt.plot(trace);  sns.distplot(prior.sample(10**5)) sns.distplot(posterior.sample(10**5)) sns.distplot(trace) sns.despine() plt.legend(labels=['Prior','Analytic Posterior', 'MCMC Posterior']) plt.xlim(-5, 7);  As you can see, even after $10^5$ samples, the MCMC posterior is not even close to the true posterior. That\u0026rsquo;s normal since the RW-MH algorithm is not very efficient: it is not a great sampler for this kind of problems. You might need a crazy number of samples before it gets close to the true posterior.\nOn the other hand, other frameworks like PyMC uses the NUTS sampler \u0026ndash; a kind of adaptive Hamiltonian Monte Carlo method. TFP supports HMC (tfp.mcmc.HamiltonianMonteCarlo), but still you might have to tune the step size and leapfrog steps parameters (this is the thing that NUTS does adaptively for you). That alone should get you closer to consistent results.\nFor more material on this subject consult Thomas Wiecki\u0026rsquo;s Blog, Bayesian Methods for Hacker book, Duke University STAT course page, and this lecture notes for a technical review of Monte Carlo Methods. The material covered here was inspired by Thomas Wiecki\u0026rsquo;s blogpost.\nIn a future blogpost I will discuss in more detail both the TFP implemetation of MCMC methods and the diagnostics of the MCMC procedure.\nBonus: PyMC3 implementation with NUTS Without going into the detail of the procedure (left as a future blogpost), below I implement the same procedure, but using pymc3 and its default sampler (NUTS)\nimport pymc3 as pm  with pm.Model() as model: mu = pm.Normal('mu', mu=4., sigma=2.) x = pm.Normal('observed', mu=mu, sigma=1., observed=observed) trace_pm = pm.sample(10000, tune=500, chains=1)  100%|██████████| 10500/10500 [00:05\u0026lt;00:00, 1971.43it/s]  sns.distplot(posterior.sample(10**5)) sns.distplot(trace_pm['mu']) sns.distplot(trace) plt.legend(labels=['Analytic Posterior', 'PyMC Posterior', 'TFP Posterior']);  As you might notice, just after $10^4$ samples, the NUTS is able to retrieve the true posterior (they are in fact indistinguishable).\n","date":1560902400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560902400,"objectID":"f2e243179073d8b1f00b7682f3c605cd","permalink":"https://lescipi.github.io/post/mcmc_tfp/","publishdate":"2019-06-19T00:00:00Z","relpermalink":"/post/mcmc_tfp/","section":"post","summary":"MCMC sampling: The Random-Walk Metropolis-Hasting algorithm explained with TensorFlow-Probability","tags":[],"title":"Bayesian Inference with MCMC","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://lescipi.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"2b27f4e7d3864cf68649ad1a1e099e78","permalink":"https://lescipi.github.io/project/statistical_concepts_r/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/statistical_concepts_r/","section":"project","summary":"Book written with `bookdown` about elementary statistical concepts.","tags":null,"title":"Statistical Concepts with R","type":"project"}]